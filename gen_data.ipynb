{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is on the left side of the white oven on ...</td>\n",
       "      <td>garbage_bin</td>\n",
       "      <td>image1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is on the left side of the fire extinguis...</td>\n",
       "      <td>table</td>\n",
       "      <td>image1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is between the the two white and black ga...</td>\n",
       "      <td>chair</td>\n",
       "      <td>image1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how many objects are between the fire extingui...</td>\n",
       "      <td>3</td>\n",
       "      <td>image1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the largest object in this picture</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>image1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question           answer image_id\n",
       "0  what is on the left side of the white oven on ...      garbage_bin   image1\n",
       "1  what is on the left side of the fire extinguis...            table   image1\n",
       "2  what is between the the two white and black ga...            chair   image1\n",
       "3  how many objects are between the fire extingui...                3   image1\n",
       "4         what is the largest object in this picture  washing_machine   image1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/VQA_English/data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of data: ['question_id', 'question', 'answer', 'image_id']\n"
     ]
    }
   ],
   "source": [
    "columns = ['question_id'] + list(data.columns)\n",
    "print(\"Columns of data:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12468/12468 [00:00<00:00, 1120758.30it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "list_data = data.values\n",
    "\n",
    "for idx, row in tqdm(enumerate(list_data), total=len(list_data)):\n",
    "    new_data.append(['quest'+str(idx+1), row[0], row[1], row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_df = pd.DataFrame(new_data, columns=columns)\n",
    "new_data_df.to_csv('dataset/VQA_English/new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_df = pd.read_csv('dataset/VQA_English/new_data.csv').values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.read_csv('dataset/VQA_English/data_train.csv').values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9974/9974 [00:04<00:00, 2291.43it/s]\n"
     ]
    }
   ],
   "source": [
    "new_train_data = []\n",
    "for idx, rowt in tqdm(enumerate(data_train_df), total=len(data_train_df)):\n",
    "    for jdx, row in enumerate(new_data_df):\n",
    "        if rowt[0] == row[1] and rowt[1] == row[2] and rowt[2] == row[3]:\n",
    "            new_train_data.append([row[0], rowt[0], rowt[1], rowt[2]])\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data_df = pd.DataFrame(new_train_data, columns=columns)\n",
    "new_train_data_df.to_csv('dataset/VQA_English/new_data_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [00:01<00:00, 2241.73it/s]\n"
     ]
    }
   ],
   "source": [
    "data_eval_df = pd.read_csv('dataset/VQA_English/data_eval.csv').values.tolist()\n",
    "\n",
    "new_eval_data = []\n",
    "for idx, rowt in tqdm(enumerate(data_eval_df), total=len(data_eval_df)):\n",
    "    for jdx, row in enumerate(new_data_df):\n",
    "        if rowt[0] == row[1] and rowt[1] == row[2] and rowt[2] == row[3]:\n",
    "            new_eval_data.append([row[0], rowt[0], rowt[1], rowt[2]])\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_data_df = pd.DataFrame(new_eval_data, columns=columns)\n",
    "new_eval_data_df.to_csv('dataset/VQA_English/new_data_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/Template/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as f:\n",
    "    jsfile = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'task_type', 'data_type', 'license', 'data_subtype', 'questions'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsfile.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 458752,\n",
       "  'question': 'What is this photo taken looking through?',\n",
       "  'question_id': 458752000},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What position is this man playing?',\n",
       "  'question_id': 458752001},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What color is the players shirt?',\n",
       "  'question_id': 458752002},\n",
       " {'image_id': 458752,\n",
       "  'question': 'Is this man a professional baseball player?',\n",
       "  'question_id': 458752003},\n",
       " {'image_id': 262146,\n",
       "  'question': 'What color is the snow?',\n",
       "  'question_id': 262146000}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsfile[\"questions\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "train2024_question = {\n",
    "    \"info\": {\n",
    "        \"description\": \"bonjour, je suis un panda\",\n",
    "        \"url\" : \"bonjour, je suis un panda\",\n",
    "        \"version\": \"17.0\",\n",
    "        \"year\": \"2024\",\n",
    "        \"contributor\": \"panda\",\n",
    "        \"date_created\": str(dt.now())\n",
    "    },\n",
    "    \"task_type\": \"PANDA\",\n",
    "    \"data_type\": \"pandacoco\",\n",
    "    \"license\" : {\n",
    "        \"url\" : \"bonjour, je suis un panda\",\n",
    "        \"name\" : \"bonjour, je suis un panda\"\n",
    "    },\n",
    "    \"data_subtype\": \"train2024\",\n",
    "    \"questions\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'info': {'description': 'bonjour, je suis un panda',\n",
       "  'url': 'bonjour, je suis un panda',\n",
       "  'version': '17.0',\n",
       "  'year': '2024',\n",
       "  'contributor': 'panda',\n",
       "  'date_created': '2024-07-16 11:30:42.667666'},\n",
       " 'task_type': 'PANDA',\n",
       " 'data_type': 'pandacoco',\n",
       " 'license': {'url': 'bonjour, je suis un panda',\n",
       "  'name': 'bonjour, je suis un panda'},\n",
       " 'data_subtype': 'train2024',\n",
       " 'questions': []}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2024_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9974/9974 [00:00<00:00, 1233931.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"dataset/VQA_English/new_data_train.csv\").values.tolist()\n",
    "\n",
    "for idx, row in tqdm(enumerate(train_csv), total=len(train_csv)):\n",
    "    train2024_question[\"questions\"].append({\n",
    "        \"image_id\" : row[3],\n",
    "        \"question\" : row[1],\n",
    "        \"question_id\": row[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "val2024_question = {\n",
    "    \"info\": {\n",
    "        \"description\": \"bonjour, je suis un panda\",\n",
    "        \"url\" : \"bonjour, je suis un panda\",\n",
    "        \"version\": \"17.0\",\n",
    "        \"year\": \"2024\",\n",
    "        \"contributor\": \"panda\",\n",
    "        \"date_created\": str(dt.now())\n",
    "    },\n",
    "    \"task_type\": \"PANDA\",\n",
    "    \"data_type\": \"pandacoco\",\n",
    "    \"license\" : {\n",
    "        \"url\" : \"bonjour, je suis un panda\",\n",
    "        \"name\" : \"bonjour, je suis un panda\"\n",
    "    },\n",
    "    \"data_subtype\": \"val2024\",\n",
    "    \"questions\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [00:00<00:00, 2112397.85it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_csv = pd.read_csv(\"dataset/VQA_English/new_data_eval.csv\").values.tolist()\n",
    "\n",
    "for idx, row in tqdm(enumerate(eval_csv), total=len(eval_csv)):\n",
    "    val2024_question[\"questions\"].append({\n",
    "        \"image_id\" : row[3],\n",
    "        \"question\" : row[1],\n",
    "        \"question_id\": row[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_object = json.dumps(train2024_question, indent=2)\n",
    "val_object = json.dumps(val2024_question, indent=2)\n",
    "\n",
    "with open(\"dataset/VQA_English/train2024_question.json\", \"w\") as f:\n",
    "    f.write(train_object)\n",
    "\n",
    "with open(\"dataset/VQA_English/val2024_question.json\", \"w\") as f:\n",
    "    f.write(val_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/Template/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    anno_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_type': 'what is this',\n",
       " 'multiple_choice_answer': 'net',\n",
       " 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'image_id': 458752,\n",
       " 'answer_type': 'other',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_data[\"annotations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"dataset/VQA_English/new_data_train.csv\").values.tolist()\n",
    "eval_csv = pd.read_csv(\"dataset/VQA_English/new_data_eval.csv\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_anno(df, filename):\n",
    "    new_anno_data = {\"annotations\": []}\n",
    "\n",
    "    for idx, row in tqdm(enumerate(df), total=len(df)):\n",
    "        new_anno_data[\"annotations\"].append({\n",
    "            \"answers\": [{\"answer\": row[2]}],\n",
    "            \"image_id\" : row[3],\n",
    "            \"question_id\" : row[0]\n",
    "        })\n",
    "    \n",
    "    object = json.dumps(new_anno_data, indent=2)\n",
    "\n",
    "    with open(\"dataset/VQA_English/\" + filename, \"w\") as f:\n",
    "        f.write(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9974/9974 [00:00<00:00, 1679405.38it/s]\n",
      "100%|██████████| 2494/2494 [00:00<00:00, 1412830.12it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_data_anno(train_csv, 'train2024_annotation.json')\n",
    "gen_data_anno(eval_csv, 'val2024_annotation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word:1094\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "saving_dir = \"dataset/VQA_English\"\n",
    "src_dir = \"dataset/VQA_English\"\n",
    "\n",
    "def make_q_vocab():\n",
    "\n",
    "    dataset = os.listdir(src_dir + '/Questions')\n",
    "\n",
    "    regex = re.compile(r'(\\W+)')\n",
    "    q_vocab = []\n",
    "\n",
    "    for file in dataset:\n",
    "\n",
    "        path = os.path.join(src_dir, 'Questions', file)\n",
    "        with open(path, 'r') as f:\n",
    "            q_data = json.load(f)\n",
    "        question = q_data['questions']\n",
    "        for idx, quest in enumerate(question):\n",
    "\n",
    "            split = regex.split(quest['question'].lower())\n",
    "            tmp = [w.strip() for w in split if len(w.strip()) > 0]\n",
    "            q_vocab.extend(tmp)\n",
    "\n",
    "    q_vocab = list(set(q_vocab))\n",
    "    q_vocab.sort()\n",
    "    q_vocab.insert(0, '<pad>')\n",
    "    q_vocab.insert(1, '<unk>')\n",
    "\n",
    "    if not os.path.exists(saving_dir): os.makedirs(saving_dir)\n",
    "    with open(saving_dir + '/question_vocabs.txt', 'w') as f:\n",
    "        f.writelines([v+'\\n' for v in q_vocab])\n",
    "\n",
    "    print(f\"total word:{len(q_vocab)}\")\n",
    "\n",
    "make_q_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val2024_annotation.json\n",
      "train2024_annotation.json\n",
      "The number of total words of answers: 543\n",
      "Keep top ['<unk>', '2', 'table', 'chair', '3', 'window', 'white', '1', 'photo', 'picture', 'cabinet', 'lamp', 'sofa', 'bed', '4', 'door', 'books', 'garbage_bin', 'mirror', 'brown', 'pillow', 'red', 'black', 'television', 'towel', 'clothes', 'blinds', 'curtain', 'bookshelf', 'shelves', '5', 'bag', 'blue', '6', 'paper', 'refridgerator', 'sink', 'stool', 'book', 'floor_mat', 'tissue_roll', 'microwave', 'pink', 'green', 'bottle_of_liquid', 'tissue_box', 'wall_decoration', 'bottle', 'decorative_item', 'stove', 'cup', 'printer', 'toilet', 'night_stand', 'light', 'ornamental_plant', 'box', 'vase', 'monitor', 'toy', 'bathtub', '8', '7', 'basket', 'jacket', 'telephone', 'whiteboard', 'plant', 'carton', 'ladder', 'drawer', 'papers', 'yellow', 'glass', 'computer', 'clock', 'bottle_of_hand_wash_liquid', 'gray', 'fan', 'oven', 'coffee_machine', 'remote_control', 'door_knob', 'stand', 'piano', 'purple', 'faucet', 'chandelier', 'laptop', 'bowl', 'light_switch', '10', 'dishwasher', 'fireplace', 'container', 'dvd_player', 'decoration_item', 'piano_bench', 'decorative_plate', 'shoe', 'door_way', 'rug', 'speaker', 'framed_certificate', 'cables', 'plate', 'hot_water_heater', '9', 'flower_pot', 'tissue', 'guitar', 'tea_kettle', 'fruit', 'room_divider', 'plastic_box', 'wardrobe', 'air_conditioner', 'dresser', 'toy_doll', 'bed_sheets', 'storage_rack', 'wire', 'blanket', 'knife_rack', 'coffee_table', 'soft_toy', 'machine', 'bunk_bed', 'cordless_phone', 'spot_light', 'baby_chair', 'board', 'ironing_board', 'poster_board', 'newspapers', 'cutting_board', 'mouse', 'keyboard', '11', 'fire_extinguisher', 'paper_holder', 'projector_screen', 'laundry_basket', 'bottle_of_soap', 'treadmill', 'water_dispenser', 'pen', 'candle', 'shirts_in_hanger', 'excercise_equipment', 'dog', 'kitchen_utensils', 'toaster', 'file', 'alarm_clock', 'stairs', 'napkin', 'paper_tray', 'electric_mixer', 'orange', 'heater', 'hat', 'projector', 'wall', 'person', '12', 'blackboard', 'excercise_machine', 'bookrack', 'lamp_shade', 'towel_rod', 'cloth_bag', 'stroller', 'globe', 'pen_stand', 'toys_rack', 'tv_stand', 'trolley', 'cream_tube', 'doll', 'chessboard', 'toy_chair', 'cooking_pan', 'utensil_container', 'candlestick', 'plant_pot', 'light_bulb', 'show_piece', 'display_case', 'bicycle', 'toy_house', 'sticker', 'comforter', 'bench', 'candelabra', 'hand_sanitizer', 'knife', 'bread', 'utensil', 'game_table', 'chart', 'step_stool', 'cell_phone_charger', 'toys_shelf', 'foot_rest', 'jug', 'grill', 'soap', 'glass_ware', 'water_cooler', 'cell_phone', 'vessel', 'magnet', 'ornamental_item', 'guitar_case', 'electrical_kettle', 'tablecloth', 'flower', 'plastic_tray', 'excercise_ball', 'backpack', 'air_vent', 'ball', 'ping_pong_table', 'briefcase', 'glass_container', 'pen_cup', 'hockey_stick', 'table_runner', 'pot', 'toothpaste', 'clothing_hanger', 'can', 'crib', 'pool_table', 'fire_alarm', 'map', 'shower_curtain', 'stacked_chairs', 'charger', 'oil_container', 'tray', 'spice_bottle', 'washing_machine', 'toiletries', 'toy_shelf', 'window_cover', 'paper_rack', 'toothbrush', 'toy_horse', 'bin', 'juicer', 'sheets', '14', 'counter', 'water_purifier', 'banana', 'yoga_mat', 'cream', 'tape_dispenser', 'deoderant', 'hair_dryer', 'mantel', 'clothing_detergent', 'notecards', 'hair_brush', 'desk', 'suitcase', 'cradle', 'fax_machine', 'calendar', 'banister', 'stapler', 'doll_house', 'toys_basket', 'cloth_drying_stand', 'toy_stroller', 'vacuum_cleaner', 'roll_of_toilet_paper', 'kitchen_utensil', 'knob', 'garbage_bag', 'desk_mat', 'spoon_stand', 'basketball', 'slide', 'radio', 'coat_hanger', 'bean_bag', 'umbrella', 'plastic_cup_of_coffee', 'headphones', 'wine_bottle', 'water_carboy', 'fish_tank', 'ottoman', 'electric_toothbrush', 'spice_stand', 'spice_rack', 'shaver', 'ping_pong_racket', 'magazine', 'cd_disc', 'switchbox', 'tape', 'dvd', 'stove_burner', 'mask', 'wall_stand', 'jeans', 'water_filter', 'cable_box', 'cap_stand', 'horse_toy', 'kichen_towel', 'tea_pot', 'plastic_tub', 'drawer_knob', 'shoe_rack', 'tupperware', 'foosball_table', 'hanger', 'business_cards', 'plastic_toy_container', 'bottle_of_listerine', 'envelope', 'clothing_drying_rack', 'bar_of_soap', 'water_heater', 'green_screen', 'brick', 'toaster_oven', 'hole_puncher', 'sheet_music', 'salt_shaker', 'key', 'roll_of_paper_towels', 'hooks', 'radiator', 'folder', 'wall_divider', 'wine_glass', 'cordless_telephone', 'scissor', 'salt_and_pepper', 'modem', 'perfume', 'medal', 'flatbed_scanner', 'decorative_bowl', 'hangers', 'globe_stand', 'floor', 'dish_scrubber', 'glass_rack', '16', 'napkin_dispenser', 'ipad', 'glass_pot', 'ashtray', 'pool_ball', 'lighting_track', 'glass_pane', 'display_board', 'door_way_arch', 'utensils', 'music_stereo', 'photo_album', 'file_box', '15', 'pipe', 'column', '18', 'wooden_plank', 'broom', 'toilet_paper', 'bottle_of_perfume', 'iron_box', 'basketball_hoop', 'toilet_brush', 'door_lock', 'sponge', 'toilet_paper_holder', 'stones', 'dvds', 'scale', 'case', 'coffee_pot', 'mattress', 'bicycle_helmet', 'purse', 'tricycle', 'chart_roll', 'aluminium_foil', 'indoor_fountain', 'mouse_pad', 'vessel_set', 'head_phones', 'vegetable', '13', 'plastic_chair', 'calculator', 'door_frame', 'bulb', 'baby_gate', 'file_container', 'balloon', 'makeup_brush', 'razor', 'hand_sculpture', 'lint_comb', 'range_hood', 'book_holder', 'eye_glasses', 'wooden_planks', 'sign', 'glass_box', 'router', 'coffee_grinder', 'jar', 'hand_sanitizer_dispenser', 'napkin_holder', 'pineapple', 'shelf_frame', 'wine_rack', 'trampoline', 'toy_triangle', 'incense_holder', 'toy_box', 'knobs', 'typewriter', 'measuring_cup', 'id_card', 'blender', 'toy_table', 'frying_pan', 'shower_hose', 'window_shelf', 'video_game', 'dog_toy', 'lint_roller', 'xbox', 'cork_board', 'can_of_beer', 'toy_truck', 'railing', 'shopping_baskets', 'electrical_outlet', 'toy_bin', 'cane', 'wire_board', 'kitchen_island', 'cat', 'folders', 'clothing_dryer', 'glass_set', 'orange_juicer', 'clothing_iron', 'decorative_candle', 'handle', 'certificate', 'pitcher', 'gift_wrapping_roll', 'alarm', 'casserole_dish', 'decorative_dish', 'toy_car', 'jersey', 'flask', 'hamburger_bun', 'window_seat', 'incense_candle', 'toothbrush_holder', 'fruit_basket', 'box_of_paper', 'sink_protector', 'eggplant', 'dishes', 'hand_fan', 'stamp', 'vegetable_peeler', 'pen_holder', 'crock_pot', 'lego', 'dish_brush', 'shoe_hanger', 'canister', 'coke_bottle', '2163', 'back_scrubber', 'pepper_shaker', 'flag', 'ladel', 'spatula', 'package_of_bottled_water', 'dog_bowl', 'onion', 'scarf', 'letter_stand', 'decorative_platter', 'bucket', 'stick', 'pencil', 'grandfather_clock', 'microphone_stand', 'nailclipper', 'electric_box', 'piece_of_wood', 'ornamental_pot', 'telescope', 'iphone', 'kitchen_items', 'telephone_cord', 'notebook', 'dish_rack', 'stacked_plastic_racks', 'drying_stand', 'tent', '19', 'tree_sculpture', 'furniture', 'belt', 'hanging_hooks', 'magazine_holder', 'file_holder', '22', 'hand_weight', 'hoola_hoop', 'album', 'faucet_handle', 'tin_foil', 'file_pad', 'head_phone', 'cd', 'ceiling', 'dog_cage', 'binder', 'chest', 'dog_bed', 'sheet', 'clothing_hamper', 'soap_holder', 'iron_grill', 'cat_cage'] answers into vocab\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def make_a_vocab(top_answer):\n",
    "\n",
    "    answers = defaultdict(lambda :0)\n",
    "    dataset = os.listdir(src_dir + '/Annotations')\n",
    "    for file in dataset:\n",
    "        print(file)\n",
    "        path = os.path.join(src_dir, 'Annotations', file)\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        annotations = data['annotations']\n",
    "        for label in annotations:\n",
    "            for ans in label['answers']:\n",
    "                vocab = ans['answer']\n",
    "                if re.search(r'[^\\w\\s]', vocab):\n",
    "                    continue\n",
    "                answers[vocab] += 1\n",
    "\n",
    "    answers = sorted(answers, key=answers.get, reverse= True) # sort by numbers\n",
    "    top_answers = ['<unk>'] + answers[:top_answer-1]\n",
    "    with open(saving_dir + '/annotation_vocabs.txt', 'w') as f :\n",
    "        f.writelines([ans+'\\n' for ans in top_answers])\n",
    "\n",
    "    print(f'The number of total words of answers: {len(answers)}')\n",
    "    print(f'Keep top {top_answers} answers into vocab' )\n",
    "\n",
    "make_a_vocab(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "image_dir = \"dataset/VQA_English/resize_image\"\n",
    "annotation_dir = \"dataset/VQA_English/Annotations\"\n",
    "question_dir = \"dataset/VQA_English/Questions\"\n",
    "output_dir = \"dataset/VQA_English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 253 out of 2494 answers are <unk>\n",
      "total 990 out of 9974 answers are <unk>\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(question, annotation_dir, image_dir, labeled):\n",
    "\n",
    "    with open(question, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        questions = data['questions']\n",
    "        if data['data_subtype'] == 'test-dev2015':\n",
    "            filename = 'test2015'   # images of test-dev are same as test images\n",
    "        else:\n",
    "            filename = data['data_subtype']\n",
    "\n",
    "    if labeled:\n",
    "        \n",
    "        template = annotation_dir + f\"/{filename}\" + \"_annotation.json\"\n",
    "        \n",
    "        annotation_path = glob.glob(template)[0]\n",
    "        with open(annotation_path) as f:\n",
    "            annotations = json.load(f)['annotations']\n",
    "        question_dict = {ans['question_id']: ans for ans in annotations}\n",
    "        \n",
    "    match_top_ans.unk_ans = 0\n",
    "\n",
    "    dataset = [None]*len(questions)\n",
    "    for idx, qu in enumerate(questions):\n",
    "        if (idx+1) % 10000 == 0:\n",
    "            print(f'processing {data[\"data_subtype\"]} data: {idx+1}/{len(questions)}')\n",
    "        qu_id = qu['question_id']\n",
    "        qu_sentence = qu['question']\n",
    "        qu_tokens = tokenizer(qu_sentence)\n",
    "        img_id = qu['image_id']\n",
    "        img_name = '{}.png'.format(img_id)\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "\n",
    "        info = {'img_name': img_name,\n",
    "                'img_path': img_path,\n",
    "                'qu_sentence': qu_sentence,\n",
    "                'qu_tokens': qu_tokens,\n",
    "                'qu_id': qu_id}\n",
    "\n",
    "        if labeled:\n",
    "\n",
    "            annotation_ans = question_dict[qu_id]['answers']\n",
    "            all_ans, valid_ans = match_top_ans(annotation_ans)\n",
    "            info['all_ans'] = all_ans\n",
    "            info['valid_ans'] = valid_ans\n",
    "\n",
    "        dataset[idx] = info\n",
    "\n",
    "    print(f'total {match_top_ans.unk_ans} out of {len(questions)} answers are <unk>')\n",
    "    return dataset\n",
    "\n",
    "def tokenizer(sentence):\n",
    "\n",
    "    regex = re.compile(r'(\\W+)')\n",
    "    tokens = regex.split(sentence.lower())\n",
    "    tokens = [w.strip() for w in tokens if len(w.strip()) > 0]\n",
    "    return tokens\n",
    "\n",
    "def match_top_ans(annotation_ans):\n",
    "\n",
    "    annotation_dir = output_dir + '/annotation_vocabs.txt'\n",
    "    if \"top_ans\" not in match_top_ans.__dict__:\n",
    "        with open(annotation_dir, 'r') as f:\n",
    "            match_top_ans.top_ans = {line.strip() for line in f}\n",
    "    annotation_ans = {ans['answer'] for ans in annotation_ans}\n",
    "    valid_ans = match_top_ans.top_ans & annotation_ans\n",
    "\n",
    "    if len(valid_ans) == 0:\n",
    "        valid_ans = ['<unk>']\n",
    "        match_top_ans.unk_ans += 1\n",
    "\n",
    "    return annotation_ans, valid_ans\n",
    "\n",
    "def main():\n",
    "\n",
    "    processed_data = {}\n",
    "    for file in os.listdir(question_dir):\n",
    "        datatype=\"test\"\n",
    "        if file.find(\"train\") != -1:\n",
    "            datatype = \"train\"\n",
    "        if file.find(\"val\") != -1:\n",
    "            datatype = \"val\"\n",
    "    \n",
    "        labeled = False if datatype == \"test\" else True\n",
    "        question = os.path.join(question_dir, file)\n",
    "        processed_data[datatype] = preprocessing(question, annotation_dir, image_dir, labeled)\n",
    "\n",
    "    processed_data['train-val'] = processed_data['train'] + processed_data['val']\n",
    "    for key, value in processed_data.items():\n",
    "        np.save(os.path.join(output_dir, f'{key}.npy'), np.array(value))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
